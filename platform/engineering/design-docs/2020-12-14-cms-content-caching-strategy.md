# CMS Content Caching Strategy

_Replace the previous line with the the title of your project or component and replace the following lines with your name(s), email and the date._  
**Author(s):** Eugene Doan  
**Last Updated:** December 15, 2020  
**Status:** **Draft** | In Review | Approved  
**Approvers:** Tim Wright \[ \], Demian Ginther \[ \], Michael Fleet \[ \], Dror Matalon \[ \]  

## Overview

### Objective

Systems and workflows that need to access the latest CMS content should be able to do so without advanced configuration or permissions (e.g., SOCKS proxy).
Having this access would benefit review instances, the GitHub Actions build pipeline, and local development. There may also be other use cases.
The intended audience for this document are both users and maintainers of these systems and any others that have a dependency on CMS content.

To support this functionality, this document will outline an approach to setting up a cache of the latest CMS content.
- This not intended to form a sophisticated caching policy with version management and rollback capabilities.
- Instead, the focus is on keeping the latest version the the CMS content updated on a schedule.

This document will not delve into implementation details of how various systems (builds, local development, etc.) will use the cache.

### Background
_The background section should contain information the reader needs to know to understand the problem being solved. This can be a combination of text and links to other documents._

_Do **NOT** describe the solution here. That goes in High Level Design._

A large part of the VA.gov website is static content that is derived from a Drupal-based content management system (CMS). 
Building the static pages of the site involves fetching the content either directly from the Drupal CMS or from a cache in Amazon S3 (Simple Storage Service).

The CMS is hosted in the internal VA network, so the request for its content has to traverse the Trusted Internet Connection (TIC).
- Only systems within the network have direct access to internal services like the CMS.
- External systems, such as local development machines outside of the network, can only access internal services through a SOCKS proxy.
- Publicly hosted services such as review instances or GitHub Actions workflows currently have no means of accessing internal services.

To circumvent the need for the SOCKS proxy or a direct internal connection, there is a content cache that's regularly generated by the Jenkins build pipeline.
- The build for each environment (dev, staging, prod) tries to pull the content to fully build the website.
- If the Drupal content pull is successful, the pipeline caches that content in S3. Otherwise, the build uses a previous cache.
- The cache is stored with a key derived from a combination of the environment name and a hash of the GraphQL query that's used to fetch all pages from Drupal.

There have been issues with generating the cache key (department-of-veterans-affairs/va.gov-team#16401) that forced review instances to attempt to directly pull content from Drupal and fail (department-of-veterans-affairs/va.gov-team#16637).
- Review instances are staging-like environments, with website and API integration, used by many veteran-facing services (VFS) teams to test changes within feature branches.
- From the service interruption, it was explained that review instances are in a separate virtual private cloud (VPC) than the internal VPC that hosts internal services like the CMS.
- In addition, because the review instances have configured an Internet gateway, they do not have clearance from VA to peer or otherwise connect with the internal VPC.
- Since they don't have a SOCKS proxy configuration or direct access to the CMS, review instances rely entirely on the cache to build the environment.

The problem with review instances prompted an overarching discussion around revising the content caching strategy.
The goals of that change would be to prevent similar issues and to better support systems that depend on the cache.

As alluded to earlier, direct requests for content from Drupal currently involve GraphQL queries. However, this is going to change.
- There is an ongoing project to overhaul the content build to use what is known as CMS export data, as opposed to the GraphQL query results.
- Naturally, eliminating the GraphQL queries would disrupt the current approach to caching (since the key is generated from those queries) as well as the systems that depend on it.

### High Level Design

Content will be cached in S3 with a constant key such as `latest.tar.bz2` instead of an ever-changing hash-based key.

An Amazon Web Services (AWS) Lambda function will pull the content from Drupal and sync the cache every 5 minutes.
The function will be versioned so that branches that have modified the query for the content can access the corresponding data.

Local development environments, review instances, GitHub Actions, and any other systems would be able to start using this new cache as needed.

## Specifics

### Detailed Design

The Lambda function will have dependencies on the current Drupal client code in the vets-website repo.
The handler will be packaged and uploaded to AWS in S3 (or ECR) for the function source.
- Bundle the code with webpack.
- Create a zip or container.

Handler needs to be updated when the GraphQL query changes.
Lambda should use the uploaded zip in S3 as a source.

Build the function in CI and zip it.
Get the checksum or hash of the zipped file.
Store the zipped file in an S3 bucket at a key corresponding to the checksum if it doesn't already exist.

Check the [list of function aliases](https://docs.aws.amazon.com/lambda/latest/dg/API_ListAliases.html) for an alias matching the checksum.
If an alias doesn't exist yet for the checksum:
- [Update and publish the function code](https://docs.aws.amazon.com/lambda/latest/dg/API_UpdateFunctionCode.html) using the archived function from S3.
- [Create the alias](https://docs.aws.amazon.com/lambda/latest/dg/API_CreateAlias.html) for the new version of the function.

If the current branch is **not** `master`, [synchronously invoke](https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html) the new version of the function (aliased with the checksum).

If the current branch is `master`, also get the version that the `master` alias points to. If the master alias doesn't match the version of the checksum alias:
- [Update the `master` alias](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/lambda/update-alias.html) to the version currently aliased with the checksum.
- Synchronously invoke the new version of the function (aliased with `master`).

Pull the cache from S3 that corresponds to the checksum (or `master` if on the `master` branch). Attach to workspace for the build step.

The S3 event should trigger the function to initialize the cache.
How to simulataneously use the bucket as the source and also trigger from S3 event?

### Code Location
_The path of the source code in the repository._

Handler source code, bundling, and upload
Terraform configuration for Lambda function

Initially, the Lambda function code will live in the vets-website repo at `src/platform/lambdas/content-cache.js`.

When the content build separation is complete, the function along with the Circle job that builds and updates the AWS resources, will move over to the content-build repo.

### Testing Plan

The GitHub Actions pipeline will include a job for building and updating the function. This will alert us to any issues with building the function and provisioning it in AWS.

The prototype for the function will simply download the data from the Drupal server. This is to verify that it can connect to the CMS at all.
Once that is confirmed, the function will be further developed to cache the downloaded data to S3.

When the function is fully written, we will monitor the log output in AWS CloudWatch to observe whether it works correctly.

### Logging

GitHub Actions will output logs for the build and versioning of the function.
Log output from the function (from calling `console` methods) will be stored in CloudWatch.

### Debugging
_How users can debug interactions with your system. When designing a system it's important to think about what tools you can provide to make debugging problems easier. Sometimes it's unclear whether the problem is in your system at all, so a mechanism for isolating a particular interaction and examining it to see if your system behaved as expected is very valuable. Once a system is in use, this is a great place to put tips and recipes for debugging. If this section grows too large, the mechanisms can be summarized here and individual tips can be moved to another document._

### Caveats

To be determined.

### Security Concerns

The function is not expected to take much bandwidth on the Drupal server, so there should be no concern about denial of service. It will be scheduled to make a request only every 5 or 10 minutes.

Without any user input, it should not be susceptible to malicious requests either.

### Privacy Concerns

There are no privacy concerns as no user data is involved.

### Open Questions and Risks
_This section should describe design questions that have not been decided yet, research that needs to be done and potential risks that could make make this system less effective or more difficult to implement._

_Some examples are: Should we communicate using TCP or UDP? How often do we expect our users to interrupt running jobs? This relies on an undocumented third-party API which may be turned off at any point._

_For each question you should include any relevant information you know. For risks you should include estimates of likelihood, cost if they occur and ideas for possible workarounds._

Will the function need to be provisioned in a VPC that can communicate with the Drupal server? If so, which one?
- Without specifying a VPC, a Lambda function is created in the default Amazon VPC, which is not private.
- It will likely need to live in a private VPC that can connect to the Drupal server. The utility VPC might be the appropriate one.

What's the appropriate frequency to update the content cache?
- Currently considering 10 minutes, because the local download of the data with SOCKS proxy seemed to take roughly 11 minutes.
- Since the function should be able to directly connect to the Drupal server, it should take less time than local development.
- If it turns out to take much less time, we can dial up the frequency to 5 minutes or under.

Will the runtime for the Lambda incur a large cost?
- Downloading the content can take over 10 minutes in local development, but that's through the SOCKS proxy.
- If the function runs on the magnitude of 5 minutes, is that short enough?

### Work Estimates
_Split the work into milestones that can be delivered, put them in the order that you think they should be done, and estimate roughly how much time you expect it each milestone to take. Ideally each milestone will take one week or less._

### Alternatives
_This section contains alternative solutions to the stated objective, as well as explanations for why they weren't used. In the planning stage, this section is useful for understanding the value added by the proposed solution and why particular solutions were discarded. Once the system has been implemented, this section will inform readers of alternative solutions so they can find the best system to address their needs._

#### Setting up a Terraform configuration for the Lambda function in the `devops` repo.

The function has dependencies on parts of the content build, so it would be cumbersome to integrate into the `devops` repo.

#### Using Serverless Framework or Terraform to publish the Lambda function in the `vets-website` repo.

Since we're provisioning a single function under the ownership of the FE Tools team for now, it seems unnecessary to leverage frameworks like Serverless or Terraform. 

There are some very specific versioning rules in the design that may not be straightforward to implement within these frameworks.

### Future Work
_Features you'd like to (or will need to) add but aren't required for the current release. This is a great place to speculate on potential features and performance improvements._

Replicate a similar caching approach with the CMS export data.

### Revision History
_The table below should record the major changes to this document. You don't need to add an entry for typo fixes, other small changes or changes before finishing the initial draft._

Date | Revisions Made | Author
-----|----------------|--------
Jan 5, 2020 | Initial draft | Eugene Doan
