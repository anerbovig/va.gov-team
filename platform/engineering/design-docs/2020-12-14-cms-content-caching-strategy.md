# CMS Content Caching Strategy

**Author(s):** Eugene Doan  
**Last Updated:** March 5, 2021  
**Status:** **Draft** | In Review | Approved  
**Approvers:**
- Tim Wright \[ \]
- Demian Ginther \[ \]
- Michael Fleet \[ \]
- Dror Matalon \[ \]

## Overview

### Objective

Systems and workflows that need to access the latest CMS content should be able to do so without advanced configuration or permissions (e.g., SOCKS proxy).
Having this access would benefit review instances, the GitHub Actions build pipeline, and local development. There may also be other use cases.
The intended audience for this document are both users and maintainers of these systems and any others that have a dependency on CMS content.

To support this functionality, this document will outline an approach to setting up a cache of the latest CMS content.
- This not intended to form a sophisticated caching policy with version management and rollback capabilities.
- Instead, the focus is on keeping the latest version the the CMS content updated on a schedule.

This document will not delve into implementation details of how various systems (builds, local development, etc.) will use the cache.

### Background

A large part of the VA.gov website is static content that is derived from a Drupal-based content management system (CMS). 
Building the static pages of the site involves fetching the content either directly from the Drupal CMS or from a cache in Amazon S3 (Simple Storage Service).

The CMS is hosted in the internal VA network, so the request for its content has to traverse the Trusted Internet Connection (TIC).
- Only systems within the network have direct access to internal services like the CMS.
- External systems, such as local development machines outside of the network, can only access internal services through a SOCKS proxy.
- Publicly hosted services such as review instances or GitHub Actions workflows currently have no means of accessing internal services.

To circumvent the need for the SOCKS proxy or a direct internal connection, there is a content cache that's regularly generated by the Jenkins build pipeline.
- The build for each environment (dev, staging, prod) tries to pull the content to fully build the website.
- If the Drupal content pull is successful, the pipeline caches that content in S3. Otherwise, the build uses a previous cache.
- The cache is stored with a key derived from a combination of the environment name and a hash of the GraphQL query that's used to fetch all pages from Drupal.

There have been issues with generating the cache key (department-of-veterans-affairs/va.gov-team#16401) that forced review instances to attempt to directly pull content from Drupal and fail (department-of-veterans-affairs/va.gov-team#16637).
- Review instances are staging-like environments, with website and API integration, used by many veteran-facing services (VFS) teams to test changes within feature branches.
- From the service interruption, it was explained that review instances are in a separate virtual private cloud (VPC) than the internal VPC that hosts internal services like the CMS.
- In addition, because the review instances have configured an Internet gateway, they do not have clearance from VA to peer or otherwise connect with the internal VPC.
- Since they don't have a SOCKS proxy configuration or direct access to the CMS, review instances rely entirely on the cache to build the environment.

The problem with review instances prompted an overarching discussion around revising the content caching strategy.
The goals of that change would be to prevent similar issues and to better support systems that depend on the cache.

### High Level Design

Content will be cached in S3 with a constant key such as `latest.tar.bz2` instead of an ever-changing hash-based key.

An Amazon Web Services (AWS) Lambda function will pull the content from Drupal and sync the cache periodically (ideally every 10 minutes).

The function will be versioned so that branches that have modified the query for the content can access the corresponding data.

Local development environments, review instances, GitHub Actions, and any other systems would be able to start using this new cache as needed.

## Specifics

### Detailed Design

Using parts of the current Drupal API client code in the `vets-website` repo, the Lambda function will be built with Webpack.

As part of continuous integration (CI), the function will be zipped and uploaded to AWS in S3 (or ECR) as the function source.
- Build the function and zip it.
- Get the checksum or hash of the zipped file.
- Store the zipped file in an S3 bucket at a key corresponding to the checksum if it doesn't already exist.

Check the [list of function aliases](https://docs.aws.amazon.com/lambda/latest/dg/API_ListAliases.html) for an alias matching the checksum.
If an alias doesn't exist yet for the checksum:
- [Update and publish the function code](https://docs.aws.amazon.com/lambda/latest/dg/API_UpdateFunctionCode.html) using the archived function from S3.
- [Create the alias](https://docs.aws.amazon.com/lambda/latest/dg/API_CreateAlias.html) for the new version of the function.

If the current branch is **not** `master`, [synchronously invoke](https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html) the new version of the function (aliased with the checksum).

If the current branch is `master`, also get the version that the `master` alias points to. If the master alias doesn't match the version of the checksum alias:
- [Update the `master` alias](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/lambda/update-alias.html) to the version currently aliased with the checksum.
- Synchronously invoke the new version of the function (aliased with `master`).

Pull the cache from S3 that corresponds to the checksum (or `master` if on the `master` branch) for the build step.

### Code Location

Initially, the Lambda function and its supporting code will live in the `vets-website`.
Once the content build is separated into the `content-build` repo, they will be moved there.

Handler source code, bundling, and upload (`vets-website`):
- Lambda function: `src/platform/lambdas/content-cache.js`
- Webpack: `config/webpack.content-cache.js`
- GitHub Actions: `.github/workflows/content-build.yml`
- Function upload script: `scripts/upload-content-cache.sh`

The Lambda function will be provisioned in the `devops` repo.
- Terraform: `terraform/environments/global/lambda.tf`

### Testing Plan

The GitHub Actions pipeline will include a job for building and updating the function. This will alert us to any issues with building the function and provisioning it in AWS.

The prototype for the function will simply download the data from the Drupal server. This is to verify that it can connect to the CMS at all.
Once that is confirmed, the function will be further developed to cache the downloaded data to S3.

When the function is fully written, we will monitor the log output in AWS CloudWatch to observe whether it works correctly.

### Logging

GitHub Actions will output logs for the build and versioning of the function.
Log output from the function (from calling `console` methods) will be stored in CloudWatch.

### Debugging

One can start diagnosing any issues using the logs mentioned above.

The Webpack build of the function can be run locally to debug issues with the build.

To locally validate that the code properly pulls the Drupal data, the function can be run in a script with some modifications to disable the parts that are specific to the Lambda environment.

Once the data is cached, it can be downloaded and used to build content. That build can be compared with a content build that pulls fresh data from Drupal with a SOCKS connection.

### Caveats

To be determined.

### Security Concerns

The function is not expected to take much bandwidth on the Drupal server, so there should be no concern about denial of service. It will be scheduled to make a request only every 5 or 10 minutes.

Without any user input, it should not be susceptible to malicious requests either.

### Privacy Concerns

There are no privacy concerns as no user data is involved.

### Open Questions and Risks

Will the function need to be provisioned in a VPC that can communicate with the Drupal server? If so, which one?
- Without specifying a VPC, a Lambda function is created in the default Amazon VPC, which is not private.
- It will likely need to live in a private VPC that can connect to the Drupal server. The utility VPC might be the appropriate one.

What's the appropriate frequency to update the content cache?
- Currently considering 10 minutes, because the local download of the data with SOCKS proxy seemed to take roughly 11 minutes.
- Since the function should be able to directly connect to the Drupal server, it should take less time than local development.
- If it turns out to take much less time, we can dial up the frequency to 5 minutes or under.

Will the runtime for the Lambda incur a large cost?
- Downloading the content can take over 10 minutes in local development, but that's through the SOCKS proxy.
- If the function runs on the magnitude of 5 minutes, is that short enough?

How do we want to manage the bucket storing the function source?
- Do we want to periodically clean out old functions?
- How will the storage costs grow as we keep old versions of the function around?
- As we anticipate the release of more content, there may be many changes to the function over time.
- We do not currently clean up the bucket for the `vets-website` builds, and that certainly occupies more space by many orders of magnitude.

### Work Estimates

- Prototype the function to validate that it can download the Drupal content (5 pts)
- Implement versioning for the function (5 pts)
- Validate the cached content and versioning logic (3 pts)
- Use the new cache to run the content build in GitHub Actions (2 pts)
- Use the new cache in review instances (2 pts)

### Alternatives

#### Setting up a Terraform configuration for the Lambda function in the `devops` repo.

The function has dependencies on parts of the content build, so it would be cumbersome to integrate into the `devops` repo.

#### Using Serverless Framework or Terraform to publish the Lambda function in the `vets-website` repo.

Since we're provisioning a single function under the ownership of the FE Tools team for now, it seems unnecessary to leverage frameworks like Serverless or Terraform. 

There are some very specific versioning rules in the design that may not be straightforward to implement within these frameworks.

### Future Work

The content caching workflow should be moved to the `content-build` repo when the content build separation is complete.

### Revision History

Date | Revisions Made | Author
-----|----------------|--------
March 5, 2021 | Initial draft | Eugene Doan
